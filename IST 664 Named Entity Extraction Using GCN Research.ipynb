{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Research about Named Entity Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In Diego Marcheggiani and Ivan Titov’s paper Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling, researchers found that the results of syntactic analysis and the results of semantic role labeling are mostly mirrored. Based on the above observations, researchers proposed a graph-based convolutional network(GCN) method for semantic role labeling. Use GCN to encode a syntax-dependent tree, resulting in a potential feature representation of the word in the sentence. The article observes that the GCN layer is complementary with the LSTM layer: when the GCN layer and the LSTM layer are superimposed, their performance gets the latest state-of-the-art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the behavior of the GCN model, we implemented a complete GCN model using python and tensorflow. If fact, in the open source project provided by the model, it gives a demo to user that help them to use the GCN with their own data. Follow the instructions, our research group use CoNLL 2003. as our dataet and try to use GCN to do Named Entity Extraction here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed the GCN model with our own data, we need to do preprocess on our raw dataset, CoNLL 2003 dataset contains thousands of sentences from newspaper and magazine. <br>\n",
    "But GCN model need three matrices:<br>\n",
    "First one is a N\\*N adjacency matrix that represent the relationship between each nodes, here N represent the number of nodes in a graph. <br>\n",
    "The second one is a N\\*D matrix that represent the feature vector of each node, here D is the number of features of a node. <br>\n",
    "The third matrix is an N\\*E matrix that reprensent the real class of each node, here E is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in our project the nodes in a graph is actually words in a sentence. So we use Stanford Parser to help us to get the relationship in a sentence. With the dependencies provided by Stanford Parser, the N*N adjacency matrix can be build easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tool we use in this project is word2vec, word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. With the help of this tool, for each word in a sentence, we can get a unique 300 dimensions vector to represent it, we use this vector as this word’s feature vector and build the N*D matrix. In this project, we set the length of feature vector 300 so here the value of D is 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the N*E matrix, the CoNLL 2003 gives the exact named entity of each word. Our research group use python to extract them and build them into matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is how we preprocess data in order to feed into GCN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The preprocess of GCN Entity Extraction project.\n",
    "- Provide functions that generate three matrix for GCN's augments\n",
    "- Improve functions, for sentences, dependencies, vectors and classes,\n",
    "- Use a dictionary to store them, only need to set the index of sentence.\n",
    "- Run this program to store four  dictionary in the local memory.\n",
    "- Extract needed matrix using find functions, or use operate[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get all sentences from a file. set the first element as key and others are set as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(dataset_path):\n",
    "    sentences = {}\n",
    "    indexes = []\n",
    "    for line in open(dataset_path):\n",
    "        line = line.split()\n",
    "        index = line[0]\n",
    "        sentence = line[1:]\n",
    "        sentences[index] = sentence\n",
    "        indexes.append(index)\n",
    "    return sentences, indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select sentences, return sentences that word number is greater than number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(sentences, number):\n",
    "    print(\"Reading the sentences ...\\n\")\n",
    "    sens = {}\n",
    "    for sen in sentences.items():\n",
    "        if (len(sen[1]) >= number):\n",
    "            sens[sen[0]] = sen[1]\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get dependencies from a file, set the index as key and dependency matrix as value, get the dependency matrix by the given index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDependencies(dependency_path, sentences):\n",
    "    print(\"Reading the dependencies...\\n\")\n",
    "    deps = {}\n",
    "    temp = []\n",
    "    for line in open(dependency_path):\n",
    "        line = line.split()\n",
    "        temp.append(line)\n",
    "    for i in range(len(temp)):\n",
    "        if(temp[i] != [] and temp[i][0] == \"Index\"):\n",
    "            index = temp[i][1]\n",
    "            length = len(sentences[index])\n",
    "            matrix = zeros([length, length], dtype=int8)\n",
    "            for dep in temp[i+1]:\n",
    "                dep = re.split('-|,|\\(|\\)', dep)\n",
    "                x = int(dep[2]) - 1\n",
    "                y = int(dep[4]) - 1\n",
    "                matrix[x][y] = 1\n",
    "            deps[index] = matrix\n",
    "    return deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatrix(index, dependencies):\n",
    "    return dependencies[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get word vectors from a file, use index as key and dependency matrix as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(wordvector_path, sentences):\n",
    "    print(\"Reading the vectors...\\n\")\n",
    "    vecs = {}\n",
    "    temp = []\n",
    "    for line in open(wordvector_path):\n",
    "        line = line.split()\n",
    "        temp.append(line)\n",
    "    for i in range(len(temp)):\n",
    "        print(i)\n",
    "        if(temp[i][0] == \"Index\"):\n",
    "            index = temp[i][1]\n",
    "            if(index not in sentences):\n",
    "                continue\n",
    "            length = len(sentences[index])\n",
    "            matrix = zeros([length, 300], dtype=float)\n",
    "            for j in range(length):\n",
    "                vector = temp[i+j+1]\n",
    "                if(vector[1] == \"N/A\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    for k in range(300):\n",
    "                        matrix[j][k] = float(vector[k+1])\n",
    "            vecs[index] = matrix\n",
    "    return vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findVectors(index, vectors):\n",
    "    return vectors[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get word label from a file, use index as key and label matrix as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClass(wordclass_path, sentences):\n",
    "    print(\"Reading the classes...\\n\")\n",
    "    classes = {}\n",
    "    temp = []\n",
    "    for line in open(wordclass_path):\n",
    "        line = line.replace(\"<\", \"|<\")\n",
    "        line = line.replace(\">\", \"> \")\n",
    "        line = re.split(\"\\|\", line)\n",
    "        index = line[0]\n",
    "        line = line[1:]\n",
    "        if (index not in sentences):\n",
    "            continue\n",
    "        matrix = zeros([5, len(sentences[index])], dtype=int8)\n",
    "        for item in line:\n",
    "            item = item.split()\n",
    "            sentence = np.array(sentences[index])\n",
    "            if(item[0] == \"<O>\"):\n",
    "                objects = item[1:]\n",
    "                for obj in objects:\n",
    "                    ii = np.where(sentence == obj)[0]\n",
    "                    for i in ii:\n",
    "                        matrix[0][i] = 1\n",
    "            if(item[0] == \"<ORG>\"):\n",
    "                orgs = item[1:]\n",
    "                for org in orgs:\n",
    "                    ii = np.where(sentence == org)[0]\n",
    "                    for i in ii:\n",
    "                        matrix[1][i] = 1\n",
    "            if(item[0] == \"<PER>\"):\n",
    "                persons = item[1:]\n",
    "                for person in persons:\n",
    "                    ii = np.where(sentence == person)[0]\n",
    "                    for i in ii:\n",
    "                        matrix[2][i] = 1\n",
    "            if (item[0] == \"<MISC>\"):\n",
    "                maliciouses = item[1:]\n",
    "                for malicious in maliciouses:\n",
    "                    ii = np.where(sentence == malicious)[0]\n",
    "                    for i in ii:\n",
    "                        matrix[3][i] = 1\n",
    "            if (item[0] == \"<LOC>\"):\n",
    "                locations = item[1:]\n",
    "                for location in locations:\n",
    "                    ii = np.where(sentence == location)[0]\n",
    "                    for i in ii:\n",
    "                        matrix[4][i] = 1\n",
    "        for n in range(len(sentences[index])):\n",
    "            isNone = 1\n",
    "            for m in range(5):\n",
    "                if(matrix[m][n] == 1):\n",
    "                    isNone = 0\n",
    "            if(isNone == 1):\n",
    "                matrix[0][n] = 1\n",
    "        classes[index] = matrix\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClasses(index, classes):\n",
    "    return classes[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(dependencies, vectors, classes, index):\n",
    "    temp = []\n",
    "    matrixNN = dependencies[index]\n",
    "    for i in range(len(matrixNN)):\n",
    "        for j in range(len(matrixNN[0])):\n",
    "            if(matrixNN[i][j] == 1):\n",
    "                temp.append((i, j))\n",
    "    temp = np.array(temp)\n",
    "    adj = sp.coo_matrix((np.ones(temp.shape[0]), (temp[:, 0], temp[:, 1])),\n",
    "                        shape=(matrixNN.shape[0], matrixNN.shape[0]), dtype=np.float32)\n",
    "    matrixND = vectors[index]\n",
    "    matrixNE = classes[index]\n",
    "    matrixNE = np.transpose(matrixNE)\n",
    "    return matrixND, adj, matrixNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training dataset, validation dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSplit(y):\n",
    "    idx_train = range((y.shape[0]) / 2)\n",
    "    idx_val = range((y.shape[0])/2, y.shape[0])\n",
    "    idx_test = range((y.shape[0])/2, y.shape[0])\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eample of implementing data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_set = 'data/CoNLL_sentence_train.txt'\n",
    "dependencies_set = \"output/sentence_dependency.txt\"\n",
    "wordvector_set = \"output/word_vec.txt\"\n",
    "wordclass_set = \"output/word_class.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, indexes = getSentences(sentence_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 1, value ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb'] \n",
      "key 2, value ['Peter', 'Blackburn'] \n",
      "key 3, value ['BRUSSELS', '1996-08-22'] \n"
     ]
    }
   ],
   "source": [
    "for x in list(sentences)[0:3]:\n",
    "    print (\"key {}, value {} \".format(x,  sentences[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = getDependencies(dependencies_set, sentences)\n",
    "vectors = getVectors(wordvector_set, sentences)\n",
    "classes = getClass(wordclass_set, sentences)\n",
    "MND, temp, MNE = getMatrix(dependencies, vectors, classes, indexes[19])\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = getSplit(MNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented our own model using graph convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from layers.graph import GraphConvolution\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "# Define parameters\n",
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 30\n",
    "PATIENCE = 10  # early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "# start = time.time()\n",
    "# data\n",
    "sentence_set = \"data/CoNLL_sentence_train_slim.txt\"\n",
    "dependencies_set = \"output/sentence_train_dependency.txt\"\n",
    "wordvector_set = \"output/CoNLL_word_vec_train.txt\"\n",
    "wordclass_set = \"data/CoNLL_class_train_slim.txt\"\n",
    "sentences, indexes = getSentences(sentence_set)\n",
    "# select sentences that length is greater than 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = getDependencies(dependencies_set, sentences)\n",
    "vectors = getVectors(wordvector_set, sentences)\n",
    "classes = getClass(wordclass_set, sentences)\n",
    "sentences_needed = select(sentences, 20)\n",
    "indexes_needed = sentences_needed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(A, X, y):\n",
    "    if FILTER == 'localpool':\n",
    "        \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "        # print('Using local pooling filters...')\n",
    "        # A = np.asarray(A)\n",
    "        A_ = preprocess_adj(A, SYM_NORM)\n",
    "        support = 1\n",
    "        graph = [X, A_]\n",
    "        G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "\n",
    "    elif FILTER == 'chebyshev':\n",
    "        \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
    "        # print('Using Chebyshev polynomial basis filters...')\n",
    "        L = normalized_laplacian(A, SYM_NORM)\n",
    "        L_scaled = rescale_laplacian(L)\n",
    "        T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
    "        support = MAX_DEGREE + 1\n",
    "        graph = [X] + T_k\n",
    "        G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(support)]\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid filter type.')\n",
    "\n",
    "    X_in = Input(shape=(X.shape[1],))\n",
    "\n",
    "    # Define model architecture\n",
    "    # NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "    # This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "    H = Dropout(0.85)(X_in)\n",
    "    H = GraphConvolution(25, support, activation='relu', kernel_regularizer=l2(5e-4))([H] + G)\n",
    "    H = Dropout(0.85)(H)\n",
    "    Y = GraphConvolution(y.shape[1], support, activation='softmax')([H] + G)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[X_in] + G, outputs=Y)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\n",
    "\n",
    "    return model, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGraph(A, X, y):\n",
    "    if FILTER == 'localpool':\n",
    "        \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "        # print('Using local pooling filters...')\n",
    "        # A = np.asarray(A)\n",
    "        A_ = preprocess_adj(A, SYM_NORM)\n",
    "        support = 1\n",
    "        graph = [X, A_]\n",
    "\n",
    "    elif FILTER == 'chebyshev':\n",
    "        \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
    "        # print('Using Chebyshev polynomial basis filters...')\n",
    "        L = normalized_laplacian(A, SYM_NORM)\n",
    "        L_scaled = rescale_laplacian(L)\n",
    "        T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
    "        support = MAX_DEGREE + 1\n",
    "        graph = [X] + T_k\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid filter type.')\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_train_loss = 99999\n",
    "\n",
    "X, MNN, y = getMatrix(dependencies, vectors, classes, '1')\n",
    "X, MNN, y = reshapeMatrix(X, MNN, y, 25)\n",
    "A = toTuple(MNN)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = getSplit(y)\n",
    "model, graph = buildModel(A, X, y)\n",
    "\n",
    "confusion_matrix = zeros([5, 5], dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "    train_loss_sum = 0\n",
    "    train_acc_sum = 0\n",
    "    test_loss_sum = 0\n",
    "    test_acc_sum = 0\n",
    "\n",
    "    train_loss_avg = 0\n",
    "    train_acc_avg = 0\n",
    "    test_loss_avg = 0\n",
    "    test_acc_avg = 0\n",
    "\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    process_bar = ShowProcess(len(indexes_needed))\n",
    "\n",
    "\n",
    "\n",
    "    #for index in indexes_needed:\n",
    "    # np.random.shuffle(indexes_needed)\n",
    "    for i in range(len(indexes_needed)):\n",
    "        X, MNN, y = getMatrix(dependencies, vectors, classes, indexes_needed[i])\n",
    "        X, MNN, y = reshapeMatrix(X, MNN, y, 25)\n",
    "\n",
    "        if not np.any(MNN) :\n",
    "        \tcontinue\n",
    "        A = toTuple(MNN)\n",
    "        y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = getSplit(y)\n",
    "        graph = getGraph(A, X, y)\n",
    "\n",
    "        train_loss = list()\n",
    "        train_acc = list()\n",
    "        test_loss = list()\n",
    "        test_acc = list()\n",
    "        if(i <= len(indexes_needed) * 0.8):\n",
    "            # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "            model.fit(graph, y_train, sample_weight=train_mask,\n",
    "                  batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "            # Predict on full dataset\n",
    "            preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "            for line in range(len(preds)):\n",
    "                true_index = 0\n",
    "                for lab in range(len(y_train[line])):\n",
    "                    if (y_train[line][lab] == 0):\n",
    "                        true_index += 1\n",
    "                    else:\n",
    "                        break\n",
    "                max_index = 0\n",
    "                for pos in range(len(preds[line])):\n",
    "                    if preds[line][pos] > preds[line][max_index]:\n",
    "                        max_index = pos\n",
    "                if(true_index >= 5 or max_index >= 5):\n",
    "                    continue\n",
    "                confusion_matrix[true_index][max_index] += 1\n",
    "\n",
    "            # Train / validation scores\n",
    "            train_loss, train_acc = evaluate_preds(preds, [y_train], [idx_train])\n",
    "            train_count = train_count + 1\n",
    "            train_loss_sum = train_loss_sum + train_loss[0]\n",
    "            train_acc_sum = train_acc_sum + train_acc[0]\n",
    "        else:\n",
    "            # Predict on full dataset\n",
    "            preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "            # Train / validation scores\n",
    "            test_loss, test_acc = evaluate_preds(preds, [y_train], [idx_train])\n",
    "            test_count = test_count + 1\n",
    "            test_loss_sum = test_loss_sum + test_loss[0]\n",
    "            test_acc_sum = test_acc_sum + test_acc[0]\n",
    "\n",
    "        process_bar.show_process()\n",
    "\n",
    "    train_loss_avg = train_loss_sum#/train_count\n",
    "    train_acc_avg = train_acc_sum/train_count\n",
    "    test_loss_avg = test_loss_sum#/test_count\n",
    "    test_acc_avg = test_acc_sum/test_count\n",
    "\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_loss_avg),\n",
    "          \"train_acc= {:.4f}\".format(train_acc_avg),\n",
    "          \"val_loss= {:.4f}\".format(test_loss_avg),\n",
    "          \"val_acc= {:.4f}\".format(test_acc_avg),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_loss_avg < best_train_loss:\n",
    "        best_train_loss = train_loss_avg\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "\n",
    "print(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_set = \"data/sentence.txt\"\n",
    "dependencies_set = \"output/sentence_dependency.txt\"\n",
    "wordvector_set = \"output/word_vec.txt\"\n",
    "wordclass_set = \"output/word_class.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, indexes = getSentences(sentence_set)\n",
    "dependencies = getDependencies(dependencies_set, sentences)\n",
    "vectors = getVectors(wordvector_set, sentences)\n",
    "classes = getClass(wordclass_set, sentences)\n",
    "MND, MNN, MNE = getMatrix(dependencies, vectors, classes, indexes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    key = indexes[i]\n",
    "    MNDi, MNNi, MNEi = getMatrix(dependencies, vectors, classes, indexes[i])\n",
    "    MND = vstack((MND, MNDi))\n",
    "    MNE = vstack((MNE, MNEi))\n",
    "    right_top = zeros((MNN.shape[0], MNNi.shape[1]), dtype=int8)\n",
    "    left_bottom = zeros((MNNi.shape[0], MNN.shape[1]), dtype = int8)\n",
    "    left = vstack((MNN, left_bottom))\n",
    "    right = vstack((right_top, MNNi))\n",
    "    MNN = hstack((left, right))\n",
    "adj = toTuple(MNN)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = getSplit(MNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally the program get 86.02% of accuracy on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, we discussed some mature method to do semantic role labeling. In the last part, we have shown a GCN model to do named entity extraction on CoNLL dataset. We believe that with the development of new technology such as deep learning and artificial neural network, some useful research area such as semantic role labeling and named entity extraction in natural language processing, will get a lot of help with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we got 86.02% of accuracy for our model,which is even higher than the state-of-art approach, we still want to think further about how to improve our model. One problem for our model is that all sentences used to train our model are from CoNLL 2003, which collected sentences from newspapers and magazines. We can try more different kinds of tagged datasets to train our model in order to increase its ability to adapt new datasets. In addition, We can also think that how the length and complexity of each sentence affect the accuracy of the model. Collecting longer and more complex sentences with correct tags might help us to train a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@article{kipf2016semi,\n",
    "  title={Semi-Supervised Classification with Graph Convolutional Networks},\n",
    "  author={Kipf, Thomas N and Welling, Max},\n",
    "  journal={arXiv preprint arXiv:1609.02907},\n",
    "  year={2016}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
